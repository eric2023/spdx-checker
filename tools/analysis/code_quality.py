#!/usr/bin/env python3
"""
SPDX Scanner ä»£ç è´¨é‡åˆ†æå·¥å…·

æ­¤å·¥å…·ç”¨äºåˆ†æé¡¹ç›®ä»£ç è´¨é‡ï¼ŒåŒ…æ‹¬ï¼š
- ä»£ç å¤æ‚åº¦åˆ†æ
- ä¾èµ–å…³ç³»åˆ†æ
- é‡å¤ä»£ç æ£€æµ‹
- æŠ€æœ¯å€ºåŠ¡è¯„ä¼°
- æµ‹è¯•è¦†ç›–ç‡åˆ†æ

ä½¿ç”¨æ–¹æ³•:
    python tools/analysis/code_quality.py [--output json|html|console] [--verbose]
"""

import os
import ast
import json
import argparse
from pathlib import Path
from typing import Dict, List, Set, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import re


@dataclass
class ComplexityMetric:
    """å¤æ‚åº¦æŒ‡æ ‡"""
    cyclomatic_complexity: int
    cognitive_complexity: int
    lines_of_code: int
    nesting_depth: int
    function_count: int
    class_count: int


@dataclass
class DependencyInfo:
    """ä¾èµ–ä¿¡æ¯"""
    module_name: str
    imports: List[str]
    imported_by: List[str]
    external_imports: Set[str]
    internal_imports: Set[str]


@dataclass
class CodeQualityReport:
    """ä»£ç è´¨é‡æŠ¥å‘Š"""
    timestamp: str
    project_path: str
    file_count: int
    total_lines: int
    total_functions: int
    total_classes: int
    average_complexity: float
    complexity_distribution: Dict[str, int]
    dependency_graph: Dict[str, DependencyInfo]
    duplicate_files: List[Tuple[str, str, float]]
    technical_debt_score: float
    recommendations: List[str]


class CodeQualityAnalyzer:
    """ä»£ç è´¨é‡åˆ†æå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.python_files = []
        self.dependency_graph: Dict[str, DependencyInfo] = {}
        self.complexity_metrics: Dict[str, ComplexityMetric] = {}
        self.duplicate_patterns: List[Tuple[str, str, float]] = []

    def analyze(self) -> CodeQualityReport:
        """æ‰§è¡Œå®Œæ•´çš„ä»£ç è´¨é‡åˆ†æ"""
        print("å¼€å§‹ä»£ç è´¨é‡åˆ†æ...")

        # 1. å‘ç°Pythonæ–‡ä»¶
        self._discover_python_files()

        # 2. åˆ†æå¤æ‚åº¦
        self._analyze_complexity()

        # 3. åˆ†æä¾èµ–å…³ç³»
        self._analyze_dependencies()

        # 4. æ£€æµ‹é‡å¤ä»£ç 
        self._detect_duplicates()

        # 5. è®¡ç®—æŠ€æœ¯å€ºåŠ¡è¯„åˆ†
        debt_score = self._calculate_technical_debt()

        # 6. ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations()

        # 7. ç”ŸæˆæŠ¥å‘Š
        report = CodeQualityReport(
            timestamp=datetime.now().isoformat(),
            project_path=str(self.project_root),
            file_count=len(self.python_files),
            total_lines=sum(m.lines_of_code for m in self.complexity_metrics.values()),
            total_functions=sum(m.function_count for m in self.complexity_metrics.values()),
            total_classes=sum(m.class_count for m in self.complexity_metrics.values()),
            average_complexity=sum(m.cyclomatic_complexity for m in self.complexity_metrics.values()) / max(len(self.complexity_metrics), 1),
            complexity_distribution=self._get_complexity_distribution(),
            dependency_graph=self.dependency_graph,
            duplicate_files=self.duplicate_patterns,
            technical_debt_score=debt_score,
            recommendations=recommendations
        )

        return report

    def _discover_python_files(self):
        """å‘ç°æ‰€æœ‰Pythonæ–‡ä»¶"""
        patterns = ["**/*.py"]
        for pattern in patterns:
            for file_path in self.project_root.rglob(pattern):
                # å¿½ç•¥ç‰¹å®šç›®å½•
                if any(part.startswith('.') for part in file_path.parts):
                    continue
                if 'venv' in file_path.parts or '__pycache__' in file_path.parts:
                    continue
                self.python_files.append(file_path)

        print(f"å‘ç° {len(self.python_files)} ä¸ªPythonæ–‡ä»¶")

    def _analyze_complexity(self):
        """åˆ†æä»£ç å¤æ‚åº¦"""
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)
                complexity = self._calculate_complexity(tree)
                self.complexity_metrics[str(file_path.relative_to(self.project_root))] = complexity

            except Exception as e:
                print(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _calculate_complexity(self, tree: ast.AST) -> ComplexityMetric:
        """è®¡ç®—ASTçš„å¤æ‚åº¦"""
        class ComplexityVisitor(ast.NodeVisitor):
            def __init__(self):
                self.cyclomatic_complexity = 1
                self.cognitive_complexity = 0
                self.lines_of_code = 0
                self.nesting_depth = 0
                self.max_nesting = 0
                self.function_count = 0
                self.class_count = 0
                self.current_nesting = 0

            def visit_If(self, node):
                self.cyclomatic_complexity += 1
                self.cognitive_complexity += 1 + self.current_nesting
                self.current_nesting += 1
                self.generic_visit(node)
                self.current_nesting -= 1

            def visit_While(self, node):
                self.cyclomatic_complexity += 1
                self.cognitive_complexity += 1 + self.current_nesting
                self.current_nesting += 1
                self.generic_visit(node)
                self.current_nesting -= 1

            def visit_For(self, node):
                self.cyclomatic_complexity += 1
                self.cognitive_complexity += 1 + self.current_nesting
                self.current_nesting += 1
                self.generic_visit(node)
                self.current_nesting -= 1

            def visit_ExceptHandler(self, node):
                self.cyclomatic_complexity += 1
                self.cognitive_complexity += 1 + self.current_nesting
                self.generic_visit(node)

            def visit_With(self, node):
                self.cognitive_complexity += 1 + self.current_nesting
                self.generic_visit(node)

            def visit_And(self, node):
                self.cognitive_complexity += 1
                self.generic_visit(node)

            def visit_Or(self, node):
                self.cognitive_complexity += 1
                self.generic_visit(node)

            def visit_FunctionDef(self, node):
                self.function_count += 1
                self.current_nesting += 1
                self.generic_visit(node)
                self.current_nesting -= 1
                self.max_nesting = max(self.max_nesting, self.current_nesting)

            def visit_ClassDef(self, node):
                self.class_count += 1
                self.current_nesting += 1
                self.generic_visit(node)
                self.current_nesting -= 1

            def generic_visit(self, node):
                super().generic_visit(node)

        visitor = ComplexityVisitor()
        visitor.visit(tree)

        return ComplexityMetric(
            cyclomatic_complexity=visitor.cyclomatic_complexity,
            cognitive_complexity=visitor.cognitive_complexity,
            lines_of_code=len([line for line in ast.get_source_segment('').split('\\n') if line.strip()]) if hasattr(ast, 'get_source_segment') else 0,
            nesting_depth=visitor.max_nesting,
            function_count=visitor.function_count,
            class_count=visitor.class_count
        )

    def _analyze_dependencies(self):
        """åˆ†æä¾èµ–å…³ç³»"""
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)
                imports = self._extract_imports(tree)
                relative_path = str(file_path.relative_to(self.project_root))

                self.dependency_graph[relative_path] = DependencyInfo(
                    module_name=relative_path,
                    imports=imports,
                    imported_by=[],  # å°†åœ¨ä¸‹ä¸€æ­¥å¡«å……
                    external_imports=set(),
                    internal_imports=set()
                )

            except Exception as e:
                print(f"åˆ†æä¾èµ–å¤±è´¥ {file_path}: {e}")

        # å¡«å……å¯¼å…¥è€…å’Œåˆ†ç±»å¯¼å…¥
        for module, info in self.dependency_graph.items():
            for imp in info.imports:
                if imp.startswith('.'):
                    # ç›¸å¯¹å¯¼å…¥
                    internal_import = self._resolve_relative_import(module, imp)
                    if internal_import and internal_import in self.dependency_graph:
                        info.internal_imports.add(internal_import)
                        self.dependency_graph[internal_import].imported_by.append(module)
                elif imp.startswith('spdx_scanner'):
                    # å†…éƒ¨æ¨¡å—å¯¼å…¥
                    info.internal_imports.add(imp)
                    if imp in self.dependency_graph:
                        self.dependency_graph[imp].imported_by.append(module)
                else:
                    # å¤–éƒ¨å¯¼å…¥
                    info.external_imports.add(imp)

    def _extract_imports(self, tree: ast.AST) -> List[str]:
        """æå–å¯¼å…¥è¯­å¥"""
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)
        return imports

    def _resolve_relative_import(self, current_module: str, relative_import: str) -> str:
        """è§£æç›¸å¯¹å¯¼å…¥"""
        # ç®€åŒ–çš„ç›¸å¯¹å¯¼å…¥è§£æ
        if relative_import.startswith('.'):
            depth = len(relative_import) - len(relative_import.lstrip('.'))
            module_parts = current_module.split('/')

            if depth > len(module_parts):
                return None

            base_parts = module_parts[:-depth] if depth > 0 else module_parts
            import_parts = relative_import.lstrip('.').split('.')

            return '/'.join(base_parts + import_parts) + '.py'
        return relative_import

    def _detect_duplicates(self):
        """æ£€æµ‹é‡å¤ä»£ç """
        file_contents = {}

        # æ”¶é›†æ–‡ä»¶å†…å®¹
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ç®€å•çš„é‡å¤æ£€æµ‹ - åŸºäºå‡½æ•°å’Œç±»å®šä¹‰
                functions = re.findall(r'^\\s*def\\s+(\\w+)', content, re.MULTILINE)
                classes = re.findall(r'^\\s*class\\s+(\\w+)', content, re.MULTILINE)

                signature = ','.join(sorted(functions + classes))
                if signature:
                    if signature not in file_contents:
                        file_contents[signature] = []
                    file_contents[signature].append(str(file_path.relative_to(self.project_root)))

            except Exception:
                continue

        # è¯†åˆ«é‡å¤
        for signature, files in file_contents.items():
            if len(files) > 1:
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                similarity = min(len(files) / 5.0, 1.0)  # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
                self.duplicate_patterns.append((files[0], files[1], similarity))

    def _get_complexity_distribution(self) -> Dict[str, int]:
        """è·å–å¤æ‚åº¦åˆ†å¸ƒ"""
        distribution = {
            'low': 0,      # 1-5
            'medium': 0,   # 6-10
            'high': 0,     # 11-20
            'very_high': 0 # >20
        }

        for metric in self.complexity_metrics.values():
            complexity = metric.cyclomatic_complexity
            if complexity <= 5:
                distribution['low'] += 1
            elif complexity <= 10:
                distribution['medium'] += 1
            elif complexity <= 20:
                distribution['high'] += 1
            else:
                distribution['very_high'] += 1

        return distribution

    def _calculate_technical_debt(self) -> float:
        """è®¡ç®—æŠ€æœ¯å€ºåŠ¡è¯„åˆ† (0-10, 10ä¸ºæœ€é«˜å€ºåŠ¡)"""
        debt_score = 0.0

        # å¤æ‚åº¦å€ºåŠ¡
        high_complexity = sum(1 for m in self.complexity_metrics.values() if m.cyclomatic_complexity > 10)
        if high_complexity > 0:
            debt_score += min(high_complexity / len(self.complexity_metrics) * 10, 3)

        # ä¾èµ–å€ºåŠ¡
        problematic_deps = 0
        for info in self.dependency_graph.values():
            if len(info.imported_by) > 5:  # è¿‡åº¦è€¦åˆ
                problematic_deps += 1
        if problematic_deps > 0:
            debt_score += min(problematic_deps / len(self.dependency_graph) * 5, 2)

        # é‡å¤ä»£ç å€ºåŠ¡
        debt_score += len(self.duplicate_patterns) * 0.5

        return min(debt_score, 10.0)

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºå¤æ‚åº¦çš„å»ºè®®
        high_complexity_files = [f for f, m in self.complexity_metrics.items() if m.cyclomatic_complexity > 15]
        if high_complexity_files:
            recommendations.append(f"é‡æ„é«˜å¤æ‚åº¦æ–‡ä»¶: {', '.join(high_complexity_files[:3])}")

        # åŸºäºä¾èµ–çš„å»ºè®®
        highly_coupled = [f for f, info in self.dependency_graph.items() if len(info.imported_by) > 5]
        if highly_coupled:
            recommendations.append(f"è€ƒè™‘æ‹†åˆ†é«˜åº¦è€¦åˆçš„æ¨¡å—: {', '.join(highly_coupled[:3])}")

        # åŸºäºé‡å¤ä»£ç çš„å»ºè®®
        if self.duplicate_patterns:
            recommendations.append(f"åˆå¹¶é‡å¤ä»£ç æ¨¡å¼ï¼Œå…±å‘ç° {len(self.duplicate_patterns)} å¤„")

        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºç«‹ä»£ç å®¡æŸ¥æµç¨‹",
            "å®æ–½è‡ªåŠ¨åŒ–æµ‹è¯•",
            "å®šæœŸè¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥",
            "å»ºç«‹æŠ€æœ¯å€ºåŠ¡ç®¡ç†æµç¨‹"
        ])

        return recommendations

    def export_report(self, report: CodeQualityReport, output_format: str = 'console', output_file: str = None):
        """å¯¼å‡ºæŠ¥å‘Š"""
        if output_format == 'json':
            with open(output_file or 'code_quality_report.json', 'w') as f:
                json.dump(asdict(report), f, indent=2)
            print(f"æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file or 'code_quality_report.json'}")

        elif output_format == 'html':
            self._generate_html_report(report, output_file)

        else:  # console
            self._print_console_report(report)

    def _print_console_report(self, report: CodeQualityReport):
        """æ‰“å°æ§åˆ¶å°æŠ¥å‘Š"""
        print("\\n" + "="*60)
        print("SPDX Scanner ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š")
        print("="*60)

        print(f"åˆ†ææ—¶é—´: {report.timestamp}")
        print(f"é¡¹ç›®è·¯å¾„: {report.project_path}")

        print(f"\\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  æ–‡ä»¶æ•°é‡: {report.file_count}")
        print(f"  æ€»è¡Œæ•°: {report.total_lines}")
        print(f"  å‡½æ•°æ•°é‡: {report.total_functions}")
        print(f"  ç±»æ•°é‡: {report.total_classes}")

        print(f"\\nğŸ§® å¤æ‚åº¦åˆ†æ:")
        print(f"  å¹³å‡å¤æ‚åº¦: {report.average_complexity:.2f}")
        print("  å¤æ‚åº¦åˆ†å¸ƒ:")
        for level, count in report.complexity_distribution.items():
            print(f"    {level}: {count} ä¸ªæ–‡ä»¶")

        print(f"\\nğŸ”— ä¾èµ–å…³ç³»:")
        print(f"  åˆ†æäº† {len(report.dependency_graph)} ä¸ªæ¨¡å—çš„ä¾èµ–å…³ç³»")

        print(f"\\nğŸ”„ é‡å¤ä»£ç :")
        if report.duplicate_files:
            print(f"  å‘ç° {len(report.duplicate_files)} å¤„é‡å¤ä»£ç ")
            for file1, file2, similarity in report.duplicate_files[:3]:
                print(f"    {file1} <-> {file2} (ç›¸ä¼¼åº¦: {similarity:.2f})")
        else:
            print("  æœªå‘ç°é‡å¤ä»£ç ")

        print(f"\\nâš ï¸  æŠ€æœ¯å€ºåŠ¡è¯„åˆ†: {report.technical_debt_score:.1f}/10")

        print(f"\\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report.recommendations, 1):
            print(f"  {i}. {rec}")

        print("="*60)

    def _generate_html_report(self, report: CodeQualityReport, output_file: str = None):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>SPDX Scanner ä»£ç è´¨é‡æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background-color: #f5f5f5; padding: 20px; border-radius: 5px; }}
        .section {{ margin: 20px 0; }}
        .metric {{ background-color: #e9f7ef; padding: 10px; margin: 10px 0; border-radius: 3px; }}
        .recommendation {{ background-color: #fff3cd; padding: 10px; margin: 10px 0; border-radius: 3px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>SPDX Scanner ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {report.timestamp}</p>
        <p>é¡¹ç›®è·¯å¾„: {report.project_path}</p>
    </div>

    <div class="section">
        <h2>åŸºæœ¬ç»Ÿè®¡</h2>
        <table>
            <tr><th>æŒ‡æ ‡</th><th>å€¼</th></tr>
            <tr><td>æ–‡ä»¶æ•°é‡</td><td>{report.file_count}</td></tr>
            <tr><td>æ€»è¡Œæ•°</td><td>{report.total_lines}</td></tr>
            <tr><td>å‡½æ•°æ•°é‡</td><td>{report.total_functions}</td></tr>
            <tr><td>ç±»æ•°é‡</td><td>{report.total_classes}</td></tr>
            <tr><td>å¹³å‡å¤æ‚åº¦</td><td>{report.average_complexity:.2f}</td></tr>
            <tr><td>æŠ€æœ¯å€ºåŠ¡è¯„åˆ†</td><td>{report.technical_debt_score:.1f}/10</td></tr>
        </table>
    </div>

    <div class="section">
        <h2>å¤æ‚åº¦åˆ†å¸ƒ</h2>
        <table>
            <tr><th>å¤æ‚åº¦ç­‰çº§</th><th>æ–‡ä»¶æ•°é‡</th></tr>
            <tr><td>ä½ (1-5)</td><td>{report.complexity_distribution['low']}</td></tr>
            <tr><td>ä¸­ (6-10)</td><td>{report.complexity_distribution['medium']}</td></tr>
            <tr><td>é«˜ (11-20)</td><td>{report.complexity_distribution['high']}</td></tr>
            <tr><td>å¾ˆé«˜ (>20)</td><td>{report.complexity_distribution['very_high']}</td></tr>
        </table>
    </div>

    <div class="section">
        <h2>æ”¹è¿›å»ºè®®</h2>
        {''.join(f'<div class="recommendation">{rec}</div>' for rec in report.recommendations)}
    </div>
</body>
</html>
        """

        with open(output_file or 'code_quality_report.html', 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file or 'code_quality_report.html'}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="SPDX Scannerä»£ç è´¨é‡åˆ†æå·¥å…·")
    parser.add_argument("--output", choices=['json', 'html', 'console'], default='console',
                       help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: console)")
    parser.add_argument("--output-file", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    # æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
    project_root = Path.cwd()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = CodeQualityAnalyzer(project_root)

    # æ‰§è¡Œåˆ†æ
    report = analyzer.analyze()

    # å¯¼å‡ºæŠ¥å‘Š
    analyzer.export_report(report, args.output, args.output_file)


if __name__ == "__main__":
    main()
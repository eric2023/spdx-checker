#!/usr/bin/env python3
"""
ä»£ç è´¨é‡æ£€æŸ¥å™¨

æ•´åˆç°æœ‰çš„ä»£ç è´¨é‡åˆ†æåŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„ä»£ç è´¨é‡éªŒè¯æ¥å£ã€‚
"""

import ast
import sys
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Any, Tuple
from dataclasses import dataclass
import tempfile
import subprocess

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œsrcåˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))


@dataclass
class CodeQualityResult:
    """ä»£ç è´¨é‡éªŒè¯ç»“æœ"""
    status: str  # 'PASS', 'FAIL', 'WARNING'
    score: float  # 0-10çš„è´¨é‡è¯„åˆ†
    metrics: Dict[str, Any]
    issues: List[Dict[str, Any]]
    recommendations: List[str]
    files_analyzed: int
    total_lines: int
    average_complexity: float


class CodeQualityChecker:
    """ä»£ç è´¨é‡æ£€æŸ¥å™¨"""

    def __init__(self, project_root: Path, config: Dict[str, Any]):
        self.project_root = project_root
        self.config = config
        self.python_files = []

        # è´¨é‡é˜ˆå€¼
        self.max_complexity = config.get('max_complexity', 15)
        self.min_coverage = config.get('min_coverage', 0.80)
        self.max_function_length = config.get('max_function_length', 50)
        self.max_class_length = config.get('max_class_length', 200)

    def analyze(self) -> CodeQualityResult:
        """æ‰§è¡Œå®Œæ•´çš„ä»£ç è´¨é‡åˆ†æ"""
        print("ğŸ” å¼€å§‹ä»£ç è´¨é‡åˆ†æ...")

        # 1. å‘ç°Pythonæ–‡ä»¶
        self._discover_python_files()

        # 2. åˆ†æä»£ç æŒ‡æ ‡
        metrics = self._analyze_metrics()

        # 3. æ£€æŸ¥ä»£ç è´¨é‡
        issues = self._check_quality_issues()

        # 4. è¿è¡Œå¤–éƒ¨å·¥å…·æ£€æŸ¥
        external_issues = self._run_external_checks()

        # 5. è®¡ç®—æ•´ä½“è¯„åˆ†
        score = self._calculate_quality_score(metrics, issues, external_issues)

        # 6. ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(metrics, issues, external_issues)

        # 7. ç¡®å®šçŠ¶æ€
        status = self._determine_status(score, issues, external_issues)

        return CodeQualityResult(
            status=status,
            score=score,
            metrics=metrics,
            issues=issues + external_issues,
            recommendations=recommendations,
            files_analyzed=len(self.python_files),
            total_lines=metrics.get('total_lines', 0),
            average_complexity=metrics.get('average_complexity', 0.0)
        )

    def _discover_python_files(self):
        """å‘ç°Pythonæ–‡ä»¶"""
        patterns = ["src/**/*.py", "tests/**/*.py", "tools/**/*.py"]
        self.python_files = []

        for pattern in patterns:
            for file_path in self.project_root.rglob(pattern):
                # å¿½ç•¥ç‰¹å®šç›®å½•
                if any(part.startswith('.') for part in file_path.parts):
                    continue
                if 'venv' in file_path.parts or '__pycache__' in file_path.parts:
                    continue
                self.python_files.append(file_path)

        print(f"  å‘ç° {len(self.python_files)} ä¸ªPythonæ–‡ä»¶")

    def _analyze_metrics(self) -> Dict[str, Any]:
        """åˆ†æä»£ç æŒ‡æ ‡"""
        metrics = {
            'total_files': len(self.python_files),
            'total_lines': 0,
            'total_functions': 0,
            'total_classes': 0,
            'complexity_metrics': {},
            'file_metrics': {},
            'average_complexity': 0.0,
            'complexity_distribution': {
                'low': 0,      # 1-5
                'medium': 0,   # 6-10
                'high': 0,     # 11-15
                'very_high': 0 # >15
            }
        }

        total_complexity = 0
        analyzed_files = 0

        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                file_metrics = self._analyze_file(content, file_path)
                metrics['file_metrics'][str(file_path.relative_to(self.project_root))] = file_metrics

                metrics['total_lines'] += file_metrics['lines_of_code']
                metrics['total_functions'] += file_metrics['function_count']
                metrics['total_classes'] += file_metrics['class_count']

                # å¤æ‚åº¦åˆ†æ
                if 'cyclomatic_complexity' in file_metrics:
                    complexity = file_metrics['cyclomatic_complexity']
                    total_complexity += complexity

                    if complexity <= 5:
                        metrics['complexity_distribution']['low'] += 1
                    elif complexity <= 10:
                        metrics['complexity_distribution']['medium'] += 1
                    elif complexity <= 15:
                        metrics['complexity_distribution']['high'] += 1
                    else:
                        metrics['complexity_distribution']['very_high'] += 1

                analyzed_files += 1

            except Exception as e:
                print(f"    åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                continue

        if analyzed_files > 0:
            metrics['average_complexity'] = total_complexity / analyzed_files

        print(f"  åˆ†æäº† {analyzed_files} ä¸ªæ–‡ä»¶")
        print(f"  å¹³å‡å¤æ‚åº¦: {metrics['average_complexity']:.2f}")

        return metrics

    def _analyze_file(self, content: str, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            tree = ast.parse(content)
            lines_of_code = len([line for line in content.split('\n') if line.strip() and not line.strip().startswith('#')])

            # ç»Ÿè®¡å‡½æ•°å’Œç±»
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]

            # è®¡ç®—å¤æ‚åº¦
            complexity_visitor = ComplexityVisitor()
            complexity_visitor.visit(tree)

            return {
                'lines_of_code': lines_of_code,
                'function_count': len(functions),
                'class_count': len(classes),
                'cyclomatic_complexity': complexity_visitor.cyclomatic_complexity,
                'cognitive_complexity': complexity_visitor.cognitive_complexity,
                'max_function_length': max([len(inspect.getsource(node).split('\n')) for node in functions], default=0),
                'max_class_length': max([len(inspect.getsource(node).split('\n')) for node in classes], default=0),
                'nesting_depth': complexity_visitor.max_nesting,
                'import_count': len([node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))])
            }
        except Exception as e:
            return {
                'lines_of_code': len(content.split('\n')),
                'function_count': 0,
                'class_count': 0,
                'error': str(e)
            }

    def _check_quality_issues(self) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ä»£ç è´¨é‡é—®é¢˜"""
        issues = []

        # æ£€æŸ¥é«˜å¤æ‚åº¦æ–‡ä»¶
        for file_path, metrics in self._get_metrics().items():
            complexity = metrics.get('cyclomatic_complexity', 0)
            if complexity > self.max_complexity:
                issues.append({
                    'type': 'high_complexity',
                    'file': file_path,
                    'severity': 'HIGH' if complexity > self.max_complexity * 2 else 'MEDIUM',
                    'message': f'æ–‡ä»¶å¤æ‚åº¦è¿‡é«˜: {complexity} (é˜ˆå€¼: {self.max_complexity})',
                    'metric': complexity,
                    'threshold': self.max_complexity
                })

            # æ£€æŸ¥è¿‡é•¿çš„å‡½æ•°
            function_length = metrics.get('max_function_length', 0)
            if function_length > self.max_function_length:
                issues.append({
                    'type': 'long_function',
                    'file': file_path,
                    'severity': 'MEDIUM',
                    'message': f'å‡½æ•°è¿‡é•¿: {function_length} è¡Œ (é˜ˆå€¼: {self.max_function_length})',
                    'metric': function_length,
                    'threshold': self.max_function_length
                })

            # æ£€æŸ¥è¿‡é•¿çš„ç±»
            class_length = metrics.get('max_class_length', 0)
            if class_length > self.max_class_length:
                issues.append({
                    'type': 'long_class',
                    'file': file_path,
                    'severity': 'MEDIUM',
                    'message': f'ç±»è¿‡é•¿: {class_length} è¡Œ (é˜ˆå€¼: {self.max_class_length})',
                    'metric': class_length,
                    'threshold': self.max_class_length
                })

            # æ£€æŸ¥è¿‡æ·±çš„åµŒå¥—
            nesting_depth = metrics.get('nesting_depth', 0)
            if nesting_depth > 4:
                issues.append({
                    'type': 'deep_nesting',
                    'file': file_path,
                    'severity': 'MEDIUM',
                    'message': f'åµŒå¥—è¿‡æ·±: {nesting_depth} å±‚ (é˜ˆå€¼: 4)',
                    'metric': nesting_depth,
                    'threshold': 4
                })

        # æ£€æŸ¥ä»£ç é‡å¤
        if self.config.get('check_duplicates', True):
            duplicate_issues = self._check_code_duplicates()
            issues.extend(duplicate_issues)

        # æ£€æŸ¥ä¾èµ–å…³ç³»
        if self.config.get('check_dependencies', True):
            dependency_issues = self._check_dependency_issues()
            issues.extend(dependency_issues)

        return issues

    def _check_code_duplicates(self) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ä»£ç é‡å¤"""
        issues = []

        # ç®€åŒ–çš„é‡å¤ä»£ç æ£€æµ‹
        function_signatures = {}
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                functions = re.findall(r'^\s*def\s+(\w+)', content, re.MULTILINE)
                for func_name in functions:
                    if func_name not in function_signatures:
                        function_signatures[func_name] = []
                    function_signatures[func_name].append(file_path)

            except Exception:
                continue

        # æŸ¥æ‰¾é‡å¤çš„å‡½æ•°å
        # Pythonæ ‡å‡†å‡½æ•°åç™½åå• - è¿™äº›å‡½æ•°åœ¨å¤šä¸ªæ–‡ä»¶ä¸­é‡å¤æ˜¯æ­£å¸¸çš„
        python_standard_functions = {
            '__init__', '__str__', '__repr__', '__len__', '__getitem__', '__setitem__',
            '__iter__', '__contains__', '__eq__', '__ne__', '__lt__', '__le__',
            '__gt__', '__ge__', '__hash__', '__bool__', '__call__', '__post_init__',
            'main', 'run', 'create_default', 'validate', 'is_valid', 'get_supported_languages',
            'is_language_supported', 'match_file', '_is_comment_line', 'parse_file',
            'scan_file', 'test', 'setup', 'check', 'build', 'clean', 'to_dict', 'from_dict',
            '_create_backup', '_is_valid_copyright_format', '_is_valid_spdx_version',
            'get_file_extension', 'get_summary', 'generate', 'generate_report', '_write_header', '_write_summary',
            '_write_details', '_write_footer', 'add_result', 'has_minimal_info', 'was_corrected',
            'needs_correction', 'is_empty', 'is_binary', 'get_copyright_years', 'analyze',
            '_discover_python_files', '_generate_recommendations',
            # ASTè®¿é—®å™¨æ–¹æ³• - ç”¨äºASTéå†çš„æ ‡å‡†æ–¹æ³•å
            'visit_If', 'visit_While', 'visit_For', 'visit_ExceptHandler', 'visit_With', 'visit_FunctionDef',
            'visit_ClassDef', 'visit_Module', 'visit_Import', 'visit_ImportFrom',
            # æµ‹è¯•å‡½æ•°åç™½åå• - æµ‹è¯•æ–‡ä»¶ä¸­å¸¸è§çš„å‡½æ•°å
            'test_get_file_extension', 'test_validation_result_summary', 'test_get_supported_languages',
            'test_is_language_supported', 'test_*',
            # æŠ¥å‘Šç”Ÿæˆç›¸å…³çš„æ ‡å‡†å‡½æ•°å
            '_generate_html_report', '_generate_console_report', '_generate_json_report', '_generate_markdown_report',
            '_setup_logging', '_format_status',
            # éªŒè¯ç›¸å…³çš„æ ‡å‡†å‡½æ•°å
            'verify_all',
            # å¸¸è§çš„å·¥å…·å‡½æ•°å
            '_load_default_config', '_determine_status', '_calculate_quality_score', '_check_quality_issues',
            '_run_external_checks', '_analyze_metrics', '_detect_circular_imports'
        }

        for func_name, files in function_signatures.items():
            # è·³è¿‡Pythonæ ‡å‡†å‡½æ•°å
            if func_name in python_standard_functions:
                continue

            if len(files) > 1:
                issues.append({
                    'type': 'duplicate_function',
                    'function': func_name,
                    'files': [str(f) for f in files],
                    'severity': 'LOW',
                    'message': f'å‘ç°é‡å¤å‡½æ•°å: {func_name} åœ¨ {len(files)} ä¸ªæ–‡ä»¶ä¸­'
                })

        return issues

    def _check_dependency_issues(self) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ä¾èµ–å…³ç³»é—®é¢˜"""
        issues = []

        # æ£€æŸ¥å¾ªç¯å¯¼å…¥
        circular_imports = self._detect_circular_imports()
        for cycle in circular_imports:
            issues.append({
                'type': 'circular_import',
                'cycle': cycle,
                'severity': 'HIGH',
                'message': f'æ£€æµ‹åˆ°å¾ªç¯å¯¼å…¥: {" -> ".join(cycle)}'
            })

        return issues

    def _detect_circular_imports(self) -> List[List[str]]:
        """æ£€æµ‹å¾ªç¯å¯¼å…¥ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®Œæ•´çš„å¾ªç¯å¯¼å…¥æ£€æµ‹éœ€è¦æ›´å¤æ‚çš„åˆ†æ
        import_graph = {}
        circular_imports = []

        # æ„å»ºç®€å•çš„å¯¼å…¥å›¾
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)
                imports = []
                for node in ast.walk(tree):
                    if isinstance(node, ast.ImportFrom) and node.module:
                        if node.module.startswith('spdx_scanner'):
                            imports.append(node.module)
                    elif isinstance(node, ast.Import):
                        for alias in node.names:
                            if alias.name.startswith('spdx_scanner'):
                                imports.append(alias.name)

                import_graph[str(file_path.relative_to(self.project_root))] = imports

            except Exception:
                continue

        # ç®€åŒ–çš„å¾ªç¯æ£€æµ‹
        visited = set()
        rec_stack = set()

        def has_cycle(node):
            visited.add(node)
            rec_stack.add(node)

            for neighbor in import_graph.get(node, []):
                if neighbor not in visited:
                    if has_cycle(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True

            rec_stack.remove(node)
            return False

        for node in import_graph:
            if node not in visited:
                if has_cycle(node):
                    # ç®€åŒ–å¤„ç†ï¼Œè¿”å›åŸºæœ¬å¾ªç¯ä¿¡æ¯
                    circular_imports.append([node, "cycle_detected"])

        return circular_imports

    def _run_external_checks(self) -> List[Dict[str, Any]]:
        """è¿è¡Œå¤–éƒ¨ä»£ç è´¨é‡æ£€æŸ¥å·¥å…·"""
        issues = []

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿è¡Œå¤–éƒ¨å·¥å…·
        if self._check_tool_available('flake8'):
            flake8_issues = self._run_flake8()
            issues.extend(flake8_issues)

        if self._check_tool_available('mypy'):
            mypy_issues = self._run_mypy()
            issues.extend(mypy_issues)

        if self._check_tool_available('black'):
            black_issues = self._run_black_check()
            issues.extend(black_issues)

        return issues

    def _check_tool_available(self, tool: str) -> bool:
        """æ£€æŸ¥å¤–éƒ¨å·¥å…·æ˜¯å¦å¯ç”¨"""
        try:
            subprocess.run([tool, '--version'], capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False

    def _run_flake8(self) -> List[Dict[str, Any]]:
        """è¿è¡Œflake8æ£€æŸ¥"""
        issues = []
        try:
            result = subprocess.run(
                ['flake8', 'src/', 'tests/', '--format=json'],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.stdout:
                flake8_output = json.loads(result.stdout)
                for issue in flake8_output:
                    issues.append({
                        'type': 'style_violation',
                        'tool': 'flake8',
                        'file': issue['filename'],
                        'line': issue['line_number'],
                        'column': issue['column_number'],
                        'code': issue['code'],
                        'message': issue['text'],
                        'severity': 'LOW',
                        'message': f"flake8: {issue['text']} ({issue['code']})"
                    })

        except (subprocess.TimeoutExpired, json.JSONDecodeError, Exception) as e:
            issues.append({
                'type': 'tool_error',
                'tool': 'flake8',
                'severity': 'LOW',
                'message': f'flake8æ£€æŸ¥å¤±è´¥: {str(e)}'
            })

        return issues

    def _run_mypy(self) -> List[Dict[str, Any]]:
        """è¿è¡Œmypyç±»å‹æ£€æŸ¥"""
        issues = []
        try:
            result = subprocess.run(
                ['mypy', 'src/', '--json'],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.stdout:
                mypy_output = json.loads(result.stdout)
                for file_path, file_issues in mypy_output.items():
                    for issue in file_issues:
                        issues.append({
                            'type': 'type_error',
                            'tool': 'mypy',
                            'file': file_path,
                            'line': issue.get('line', 0),
                            'message': issue.get('message', ''),
                            'severity': 'MEDIUM' if issue.get('code') == 'attr-defined' else 'LOW',
                            'message': f"mypy: {issue.get('message', '')}"
                        })

        except (subprocess.TimeoutExpired, json.JSONDecodeError, Exception) as e:
            issues.append({
                'type': 'tool_error',
                'tool': 'mypy',
                'severity': 'LOW',
                'message': f'mypyæ£€æŸ¥å¤±è´¥: {str(e)}'
            })

        return issues

    def _run_black_check(self) -> List[Dict[str, Any]]:
        """è¿è¡Œblackæ ¼å¼æ£€æŸ¥"""
        issues = []
        try:
            result = subprocess.run(
                ['black', '--check', '--diff', 'src/', 'tests/'],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode != 0:
                issues.append({
                    'type': 'format_violation',
                    'tool': 'black',
                    'severity': 'LOW',
                    'message': 'ä»£ç æ ¼å¼ä¸ç¬¦åˆblackæ ‡å‡†',
                    'details': result.stdout[:500]  # é™åˆ¶è¾“å‡ºé•¿åº¦
                })

        except (subprocess.TimeoutExpired, Exception) as e:
            issues.append({
                'type': 'tool_error',
                'tool': 'black',
                'severity': 'LOW',
                'message': f'blackæ ¼å¼æ£€æŸ¥å¤±è´¥: {str(e)}'
            })

        return issues

    def _calculate_quality_score(self, metrics: Dict, issues: List[Dict], external_issues: List[Dict]) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ† (0-10)"""
        score = 10.0

        # å¤æ‚åº¦æ‰£åˆ†
        avg_complexity = metrics.get('average_complexity', 0)
        if avg_complexity > self.max_complexity:
            score -= min((avg_complexity - self.max_complexity) * 0.5, 3.0)

        # é—®é¢˜æ‰£åˆ†
        high_severity_issues = [issue for issue in issues + external_issues if issue.get('severity') == 'HIGH']
        medium_severity_issues = [issue for issue in issues + external_issues if issue.get('severity') == 'MEDIUM']
        low_severity_issues = [issue for issue in issues + external_issues if issue.get('severity') == 'LOW']

        score -= len(high_severity_issues) * 1.0
        score -= len(medium_severity_issues) * 0.5
        score -= len(low_severity_issues) * 0.1

        return max(0.0, min(10.0, score))

    def _determine_status(self, score: float, issues: List[Dict], external_issues: List[Dict]) -> str:
        """ç¡®å®šè´¨é‡çŠ¶æ€"""
        high_severity_issues = [issue for issue in issues + external_issues if issue.get('severity') == 'HIGH']

        if high_severity_issues:
            return 'FAIL'
        elif score < 7.0:
            return 'WARNING'
        else:
            return 'PASS'

    def _generate_recommendations(self, metrics: Dict, issues: List[Dict], external_issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºè¯„åˆ†ç”Ÿæˆå»ºè®®
        score = self._calculate_quality_score(metrics, issues, external_issues)
        if score < 5.0:
            recommendations.append("ä»£ç è´¨é‡è¾ƒå·®ï¼Œå»ºè®®ä¼˜å…ˆé‡æ„é«˜å¤æ‚åº¦å’Œé•¿å‡½æ•°")
        elif score < 7.0:
            recommendations.append("ä»£ç è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®é€æ­¥æ”¹è¿›å‘ç°çš„é—®é¢˜")

        # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆå»ºè®®
        complexity_issues = [issue for issue in issues if issue.get('type') == 'high_complexity']
        if complexity_issues:
            recommendations.append(f"é‡æ„ {len(complexity_issues)} ä¸ªé«˜å¤æ‚åº¦æ–‡ä»¶ï¼Œé™ä½åœˆå¤æ‚åº¦")

        function_issues = [issue for issue in issues if issue.get('type') == 'long_function']
        if function_issues:
            recommendations.append(f"æ‹†åˆ† {len(function_issues)} ä¸ªè¿‡é•¿å‡½æ•°ï¼Œæé«˜ä»£ç å¯è¯»æ€§")

        style_issues = [issue for issue in external_issues if issue.get('type') == 'style_violation']
        if style_issues:
            recommendations.append("è¿è¡Œä»£ç æ ¼å¼åŒ–å·¥å…·ï¼ˆå¦‚blackï¼‰ç»Ÿä¸€ä»£ç é£æ ¼")

        type_issues = [issue for issue in external_issues if issue.get('type') == 'type_error']
        if type_issues:
            recommendations.append("æ·»åŠ ç±»å‹æ³¨è§£ï¼Œä½¿ç”¨mypyè¿›è¡Œç±»å‹æ£€æŸ¥")

        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºç«‹ä»£ç å®¡æŸ¥æµç¨‹ï¼Œç¡®ä¿ä»£ç è´¨é‡",
            "è®¾ç½®é¢„æäº¤é’©å­ï¼Œè‡ªåŠ¨è¿è¡Œè´¨é‡æ£€æŸ¥",
            "å®šæœŸè¿›è¡Œä»£ç é‡æ„ï¼Œæ¶ˆé™¤æŠ€æœ¯å€ºåŠ¡",
            "æé«˜æµ‹è¯•è¦†ç›–ç‡ï¼Œç¡®ä¿ä»£ç å¯é æ€§"
        ])

        return recommendations

    def _get_metrics(self) -> Dict[str, Dict]:
        """è·å–æ–‡ä»¶æŒ‡æ ‡"""
        return {}


class ComplexityVisitor(ast.NodeVisitor):
    """å¤æ‚åº¦è®¿é—®å™¨"""

    def __init__(self):
        self.cyclomatic_complexity = 1
        self.cognitive_complexity = 0
        self.max_nesting = 0
        self.current_nesting = 0

    def visit_If(self, node):
        self.cyclomatic_complexity += 1
        self.cognitive_complexity += 1 + self.current_nesting
        self._visit_with_nesting(node)

    def visit_While(self, node):
        self.cyclomatic_complexity += 1
        self.cognitive_complexity += 1 + self.current_nesting
        self._visit_with_nesting(node)

    def visit_For(self, node):
        self.cyclomatic_complexity += 1
        self.cognitive_complexity += 1 + self.current_nesting
        self._visit_with_nesting(node)

    def visit_ExceptHandler(self, node):
        self.cyclomatic_complexity += 1
        self.cognitive_complexity += 1 + self.current_nesting
        self.generic_visit(node)

    def visit_With(self, node):
        self.cognitive_complexity += 1 + self.current_nesting
        self.generic_visit(node)

    def visit_FunctionDef(self, node):
        self.current_nesting += 1
        self.generic_visit(node)
        self.current_nesting -= 1
        self.max_nesting = max(self.max_nesting, self.current_nesting)

    def _visit_with_nesting(self, node):
        self.current_nesting += 1
        self.generic_visit(node)
        self.current_nesting -= 1
        self.max_nesting = max(self.max_nesting, self.current_nesting)


# ä¿®å¤å¯¼å…¥é—®é¢˜
import inspect